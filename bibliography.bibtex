@misc{ Nobody06,
       author = "Nobody Jr",
       title = "My Article",
       year = "2006" 
	   }
	   
@book{LittleRoderickJ.A.2002SAwM,
publisher = {John Wiley & Sons, Inc.},
isbn = {9780471183860},
year = {2002},
title = {Statistical Analysis with Missing Data},
language = {eng},
address = {Hoboken, NJ, USA},
author = {Little, Roderick J. A. and Rubin, Donald B.},
keywords = {Explicit Modeling ; Implicit Modeling ; Predictive Distribution ; Single Imputation Methods},
}

@article{RubinDonaldB.1976IaMD,
issn = {00063444},
abstract = {When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
journal = {Biometrika},
pages = {581--592},
volume = {63},
publisher = {Biometrika Trust},
number = {3},
year = {1976},
title = {Inference and Missing Data},
language = {eng},
author = {Rubin, Donald B.},
keywords = {Information science -- Information management -- Data management ; Mathematics -- Applied mathematics -- Statistics ; Mathematics -- Pure mathematics -- Probability theory ; Behavioral sciences -- Psychology -- Cognitive psychology ; Mathematics -- Applied mathematics -- Statistics ; Information science -- Information management -- Information classification ; Mathematics -- Applied mathematics -- Analytics ; Mathematics -- Pure mathematics -- Probability theory ; Biological sciences -- Biology -- Physiology ; Mathematics -- Applied mathematics -- Statistics},
}

@article{SunBaoluo2017OIPW,
issn = {0162-1459},
abstract = {ABSTRACT The development of coherent missing data models to account for nonmonotone missing at random (MAR) data by inverse probability weighting (IPW) remains to date largely unresolved. As a consequence, IPW has essentially been restricted for use only in monotone MAR settings. We propose a class of models for nonmonotone missing data mechanisms that spans the MAR model, while allowing the underlying full data law to remain unrestricted. For parametric specifications within the proposed class, we introduce an unconstrained maximum likelihood estimator for estimating the missing data probabilities which is easily implemented using existing software. To circumvent potential convergence issues with this procedure, we also introduce a constrained Bayesian approach to estimate the missing data process which is guaranteed to yield inferences that respect all model restrictions. The efficiency of standard IPW estimation is improved by incorporating information from incomplete cases through an augmented estimating equation which is optimal within a large class of estimating equations. We investigate the finite-sample properties of the proposed estimators in extensive simulations and illustrate the new methodology in an application evaluating key correlates of preterm delivery for infants born to HIV-infected mothers in Botswana, Africa. Supplementary materials for this article are available online.},
journal = {Journal of the American Statistical Association},
pages = {1--11},
volume = {113},
publisher = {Taylor & Francis},
number = {521},
year = {2018},
title = {On Inverse Probability Weighting for Nonmonotone Missing at Random Data},
language = {eng},
author = {Sun, Baoluo and Tchetgen Tchetgen, Eric J.},
keywords = {Article ; Augmented Ipw ; Bayes ; Nonmonotone Missing At Random Data},
}

@article{HorvitzD.G.1952AGoS,
issn = {0162-1459},
abstract = {Abstract This paper presents a general technique for the treatment of samples drawn without replacement from finite universes when unequal selection probabilities are used. Two sampling schemes are discussed in connection with the problem of determining optimum selection probabilities according to the information available in a supplementary variable. Admittedly, these two schemes have limited application. They should prove useful, however, for the first stage of sampling with multi-stage designs, since both permit unbiased estimation of the sampling variance without resorting to additional assumptions. * Journal Paper No. J2139 of the Iowa Agricultural Experiment Station, Ames, Iowa, Project 1005. Presented to the Institute of Mathematical Statistics, March 17, 1951. * Journal Paper No. J2139 of the Iowa Agricultural Experiment Station, Ames, Iowa, Project 1005. Presented to the Institute of Mathematical Statistics, March 17, 1951.},
journal = {Journal of the American Statistical Association},
pages = {663--685},
volume = {47},
publisher = {Taylor & Francis Group},
number = {260},
year = {1952},
title = {A Generalization of Sampling Without Replacement from a Finite Universe},
author = {Horvitz, D. G. and Thompson, D. J.},
keywords = {Statistics;},
}

@article{SchaferJosephL.2002MDOV,
issn = {1082-989X},
abstract = {Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, discourage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayesian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the mainstream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art.},
journal = {Psychological Methods},
pages = {147--177},
volume = {7},
publisher = {American Psychological Association},
number = {2},
year = {2002},
title = {Missing Data: Our View of the State of the Art},
language = {eng},
author = {Schafer, Joseph L. and Graham, John W.},
keywords = {Psychology;},
}

@article{SeamanShaunR2013Roip,
issn = {0962-2802},
abstract = {<p> The simplest approach to dealing with missing data is to restrict the analysis to complete cases, i.e. individuals with no missing values. This can induce bias, however. Inverse probability weighting (IPW) is a commonly used method to correct this bias. It is also used to adjust for unequal sampling fractions in sample surveys. This article is a review of the use of IPW in epidemiological research. We describe how the bias in the complete-case analysis arises and how IPW can remove it. IPW is compared with multiple imputation (MI) and we explain why, despite MI generally being more efficient, IPW may sometimes be preferred. We discuss the choice of missingness model and methods such as weight truncation, weight stabilisation and augmented IPW. The use of IPW is illustrated on data from the 1958 British Birth Cohort. </p>},
journal = {Statistical Methods in Medical Research},
pages = {278--295},
volume = {22},
publisher = {SAGE Publications},
number = {3},
year = {2013},
title = {Review of inverse probability weighting for dealing with missing data},
language = {eng},
address = {London, England},
author = {Seaman, Shaun R and White, Ian R},
keywords = {Asymptotic Efficiency ; Doubly Robust ; Model Misspecification ; Propensity Score ; Medicine ; Statistics ; Mathematics},
}

@article{ZhangDao-Qiang2003CIDU,
issn = {1370-4621},
abstract = {There is a trend in recent machine learning community to construct a nonlinear version of a linear algorithm using the 'kernel method', e.g. Support Vector Machines (SVMs), kernel principal component analysis, kernel fisher discriminant analysis and the recent kernel clustering algorithms. In unsupervised clustering algorithms using kernel method, typically, a nonlinear mapping is used first to map the data into a potentially much higher feature space, where clustering is then performed. A drawback of these kernel clustering algorithms is thatthe clustering prototypes lie in high dimensional feature space and hence lack clear and intuitive descriptions unless using additional projection approximation from the feature to the data space as done in the existing literatures. In this paper, a novel clustering algorithm using the 'kernel method' based on the classical fuzzy clustering algorithm (FCM) is proposed and called as kernel fuzzy c-means algorithm (KFCM). KFCM adopts a new kernel-induced metric in the data space to replace the original Euclidean norm metric in FCM and the clustered prototypes still lie in the data space so that the clustering results can be reformulated and interpreted in the original space. Our analysis shows that KFCM is robust to noise and outliers and also tolerates unequal sized clusters. And finally this property is utilized to cluster incomplete data. Experiments on two artificial and one real datasets show that KFCM hasbetter clustering performance and more robust than several modifications of FCM for incomplete data clustering.},
journal = {Neural Processing Letters},
pages = {155--162},
volume = {18},
publisher = {Kluwer Academic Publishers},
number = {3},
year = {2003},
title = {Clustering Incomplete Data Using Kernel-Based Fuzzy C-means Algorithm},
language = {eng},
address = {Dordrecht},
author = {Zhang, Dao-Qiang and Chen, Song-Can},
keywords = {clustering ; fuzzy c-means ; incomplete data ; Kernel methods},
}


